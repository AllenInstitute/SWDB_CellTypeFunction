{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3734d32-6b05-446c-a9bc-6dcc73b84bf9",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "\n",
    "#### Connections vs Connection Strength\n",
    "The above analysis we did was not conditioned on connected neurons, but did not consider the strength of that connection. If you weight the distribution by different metrics of strength (either # of synapses or the summed synaptic size) how do the results change?\n",
    "\n",
    "This is a relatively short exercise which will involve using numpy and pandas to manipulate the distributions obtained in the workshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85d2c667-6e47-483c-9cfd-7dece611943c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import caveclient\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcca0df-4918-4edd-8df9-2e827a3c533b",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "### We will start with recalculating the dataframe from the workshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d959525-6528-4f17-8952-ead9e810b7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import os\n",
    "\n",
    "platstring = platform.platform()\n",
    "if ('Darwin' in platstring) or ('macOS' in platstring):\n",
    "    # macOS \n",
    "    data_root = \"/Volumes/Brain2023/\"\n",
    "elif 'Windows'  in platstring:\n",
    "    # Windows (replace with the drive letter of USB drive)\n",
    "    data_root = \"E:/\"\n",
    "elif ('amzn' in platstring):\n",
    "    # then on Code Ocean\n",
    "    data_root = \"/data/\"\n",
    "else:\n",
    "    # then your own linux platform\n",
    "    # EDIT location where you mounted hard drive\n",
    "    data_root = \"/media/$USERNAME/Brain2023/\"\n",
    "    \n",
    "data_dir = os.path.join(data_root, 'microns_in_silico')\n",
    "\n",
    "# you can just override this if the location of the data varies\n",
    "# data_dir = '/Users/forrestc/Downloads/microns_in_silico/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76889fc1-ef62-45c4-bc00-1d8029a4af28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are going to load up the data and prepare the dataframe like we did \n",
    "# in class but with fewer comments\n",
    "\n",
    "# load up the in-silico responses as a pandas dataframe from a numpy array \n",
    "resp=pd.DataFrame(np.load(os.path.join(data_dir, 'nat_resp.npy')))\n",
    "\n",
    "# load up the csv of metadata about the 104171 units\n",
    "units_df = pd.read_csv(os.path.join(data_dir, 'nat_unit.csv'))\n",
    "\n",
    "# set the index to the be the row_idx of the units_df\n",
    "resp.index = units_df['row_idx']\n",
    "\n",
    "# if we are on code ocean, the CAVEsetup helped you make your token an environment variable\n",
    "if 'amzn' in platstring:\n",
    "    client= caveclient.CAVEclient('minnie65_public', auth_token=os.environ['API_SECRET'])\n",
    "else:\n",
    "    # otherwise if you are local, then it should be saved to a file in your harddrive \n",
    "    # that the caveclient knows where to read.\n",
    "    client= caveclient.CAVEclient('minnie65_public')\n",
    "\n",
    "# lets pull out the manual coregistered neurons\n",
    "# desired_resolution describes how many nanometers you want each unit to be\n",
    "# so 1000,1000,1000 gives positions in microns for x,y and z\n",
    "coreg_df = client.materialize.query_table('coregistration_manual_v3', desired_resolution=[1000,1000,1000])\n",
    "\n",
    "# lets merge these dataframes so we get the row_idx of each coregistered unit\n",
    "# we merge on the corresponding columns, however scan was called something\n",
    "# slightly different in one csv vs the CAVE table\n",
    "coreg_in_silico=pd.merge(units_df, coreg_df, \n",
    "         left_on=['scan_session', 'scan_idx', 'unit_id'],\n",
    "          right_on=['session','scan_idx', 'unit_id'])\n",
    "# reset the index to make sure that we have the index\n",
    "coreg_in_silico.reset_index(inplace=True)\n",
    "\n",
    "# this will pull out the responses to the coregistered units\n",
    "# by using the row_idx that was provided in the metadata\n",
    "coreg_resp = resp.loc[coreg_in_silico.row_idx,:]\n",
    "\n",
    "# now with a reduced set of units, we can calculate the Pearson correlation\n",
    "# between their responses\n",
    "corr_M = np.corrcoef(coreg_resp.values)\n",
    "\n",
    "ct_df = client.materialize.query_table('aibs_soma_nuc_exc_mtype_preds_v117')\n",
    "# lets merge it on the coregistered cells with in silico responses\n",
    "# we will use the segment version is (pt_root_id) to do this\n",
    "ct_merge_df=pd.merge(coreg_in_silico.reset_index(),\n",
    "                     ct_df[['pt_root_id', 'id_ref', 'cell_type']],\n",
    "                     on='pt_root_id')\n",
    "\n",
    "# lets pull all the neurons where we can trust the axons\n",
    "# as being reasonably well reconstructed\n",
    "prf_df=client.materialize.query_table('proofreading_status_public_release', \n",
    "                                      filter_in_dict={'status_axon': ['extended', 'clean']})\n",
    "\n",
    "# how many of these are coregistered?\n",
    "clean_coreg_df = pd.merge(prf_df, coreg_in_silico, on='pt_root_id')\n",
    "\n",
    "# we need this code to work in solutions directory\n",
    "# and one up..\n",
    "if 'solutions' in os.getcwd():\n",
    "    workshop2file = '../../workshop2/all_prf_coreg_conn_v661.pkl'\n",
    "else:\n",
    "    workshop2file = '../workshop2/all_prf_coreg_conn_v661.pkl'\n",
    "all_syn_df = pd.read_pickle(workshop2file)\n",
    "\n",
    "nuc_df = client.materialize.query_view('nucleus_detection_lookup_v1', \n",
    "                                        select_columns = ['id', 'pt_root_id', 'pt_position'],\n",
    "                                        desired_resolution=[1000,1000,1000])\n",
    "\n",
    "# lets merge on the pre and post-synaptic positions of these connections\n",
    "\n",
    "# renaming the positions as pre and post depending on how we did the merge\n",
    "# and drop the duplicate id columns\n",
    "all_syn_dfm=all_syn_df.merge(nuc_df[['id', 'pt_position']], left_on='pre_nuc_id', right_on='id')\\\n",
    ".rename({'pt_position':'pre_pt_position'}, axis=1)\\\n",
    ".merge(nuc_df[['id', 'pt_position']], left_on='post_nuc_id', right_on='id')\\\n",
    ".rename({'pt_position':'post_pt_position'}, axis=1)\\\n",
    ".drop(['id_x', 'id_y'], axis=1)\n",
    "\n",
    "# now lets merge in the neurons that are coregistered with responses\n",
    "\n",
    "# we have to drop duplicates to avoid the few cells that were coregistered twice \n",
    "# being double counted\n",
    "all_syn_dfm2=all_syn_dfm.merge(coreg_in_silico[['index','target_id', 'scan_session', 'scan_idx', 'field','unit_id', 'score', 'residual']],\n",
    "                  left_on='pre_nuc_id', \n",
    "                  right_on='target_id')\\\n",
    ".merge(coreg_in_silico[['index','target_id', 'scan_session', 'scan_idx', 'field','unit_id','score', 'residual']],\n",
    "                  left_on='post_nuc_id', \n",
    "                  right_on='target_id',\n",
    "                  suffixes=['_pre', '_post'])\\\n",
    ".drop(['target_id_pre', 'target_id_post'],axis=1)\\\n",
    ".drop_duplicates(subset=['pre_nuc_id', 'post_nuc_id'])\n",
    "all_syn_dfm2\n",
    "\n",
    "# now use fancy indexing to pull out the correlation associated with each of these connections\n",
    "all_syn_dfm2['C']=corr_M[all_syn_dfm2.index_pre, all_syn_dfm2.index_post]\n",
    "\n",
    "\n",
    "# now lets merge in our cell type calls\n",
    "# by using suffixes we will name the pre and post synaptic cell type \n",
    "# differently\n",
    "all_syn_dfm3=all_syn_dfm2.merge(ct_df[['target_id', 'cell_type']],\n",
    "                  left_on='pre_nuc_id', \n",
    "                  right_on='target_id')\\\n",
    ".merge(ct_df[['target_id', 'cell_type']],\n",
    "                  left_on='post_nuc_id', \n",
    "                  right_on='target_id',\n",
    "                  suffixes=['_pre', '_post'])\\\n",
    ".drop(['target_id_pre', 'target_id_post'],axis=1)\\\n",
    ".drop_duplicates(subset=['pre_nuc_id', 'post_nuc_id'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "574029a8-0fbf-4953-a359-be187915624a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pre_pt_root_id</th>\n",
       "      <th>post_pt_root_id</th>\n",
       "      <th>n_syn</th>\n",
       "      <th>sum_size</th>\n",
       "      <th>C</th>\n",
       "      <th>cell_type_pre</th>\n",
       "      <th>cell_type_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>864691135927049742</td>\n",
       "      <td>864691136310417242</td>\n",
       "      <td>1</td>\n",
       "      <td>1732</td>\n",
       "      <td>0.169119</td>\n",
       "      <td>L5ET</td>\n",
       "      <td>L5ET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>864691136228183377</td>\n",
       "      <td>864691136310417242</td>\n",
       "      <td>1</td>\n",
       "      <td>7604</td>\n",
       "      <td>0.010837</td>\n",
       "      <td>L4c</td>\n",
       "      <td>L5ET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>864691135155894884</td>\n",
       "      <td>864691136310417242</td>\n",
       "      <td>1</td>\n",
       "      <td>9404</td>\n",
       "      <td>0.040026</td>\n",
       "      <td>L2a</td>\n",
       "      <td>L5ET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>864691135591041291</td>\n",
       "      <td>864691136310417242</td>\n",
       "      <td>2</td>\n",
       "      <td>25900</td>\n",
       "      <td>0.145264</td>\n",
       "      <td>L2a</td>\n",
       "      <td>L5ET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864691136194822888</td>\n",
       "      <td>864691136310417242</td>\n",
       "      <td>1</td>\n",
       "      <td>9972</td>\n",
       "      <td>0.191115</td>\n",
       "      <td>L4a</td>\n",
       "      <td>L5ET</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pre_pt_root_id     post_pt_root_id  n_syn  sum_size         C  \\\n",
       "0  864691135927049742  864691136310417242      1      1732  0.169119   \n",
       "1  864691136228183377  864691136310417242      1      7604  0.010837   \n",
       "2  864691135155894884  864691136310417242      1      9404  0.040026   \n",
       "3  864691135591041291  864691136310417242      2     25900  0.145264   \n",
       "4  864691136194822888  864691136310417242      1      9972  0.191115   \n",
       "\n",
       "  cell_type_pre cell_type_post  \n",
       "0          L5ET           L5ET  \n",
       "1           L4c           L5ET  \n",
       "2           L2a           L5ET  \n",
       "3           L2a           L5ET  \n",
       "4           L4a           L5ET  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets check we got the same thing\n",
    "all_syn_dfm3[['pre_pt_root_id', 'post_pt_root_id', 'n_syn',\n",
    "              'sum_size', 'C', 'cell_type_pre', 'cell_type_post']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "336a2790-afc8-4735-b021-36de41b66c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets replot the connected vs all connection distribution we had in the workshop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88ef48f7-82ec-41d1-882c-9d18fc0a29cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use np.histogram functionality to calculate a distribution\n",
    "# which is weighted by different factors.  Consult the documentation using the ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73d2fe82-74f3-4d78-850f-4d3690a097f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a weight based on the number of synapses in the connection\n",
    "\n",
    "\n",
    "# construct a weight based on the summed synapse size in the connection\n",
    "\n",
    "\n",
    "# construct a weight based on the log of the summed synapse size in the connection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26fd3c34-4b91-455d-9ff5-a7c10db65eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a distribution of each of these weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79edd9b3-1555-4925-baf9-24ee66f7ee1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use numpy histogram to make distributions of correlations which are weighed by each of these 3 metrics\n",
    "# plus the unweighted distribution and the overall distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e939ea4-fd31-4f76-8220-618041292a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all 4 distributions relative to one another\n",
    "# hint use plt or ax .stairs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9b11dd3-e8c5-41f1-84a0-2c26dad769f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the mean of these distributions\n",
    "# which has the largest effect on the distribution?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869fe7dc-4d09-4cc7-b504-e347cbfbd4fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccae81d4-4d34-4ffa-b8f4-1a87e260b1e7",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "\n",
    "#### Extensions/Project Ideas\n",
    "\n",
    "How can we calculate whether these differences are significant? \n",
    "   1. Standard error \n",
    "   2. Weighted KS tests\n",
    "\n",
    "\n",
    "We can incorporate this weighted view of connection strength into the single cell effects, and/or spatial effects.\n",
    "\n",
    "There are more sophisticated metrics of structural weight that could be applied. For example, taking into account how far the synapses are from the soma of the target cell.  Does that increase or decrease the effect size?\n",
    "\n",
    "One should look at the extremes of this distribution, strong synapses that don't have correlations, weak synapses that do.  Do you notice anything different about them? Spine apparatus, spine vs shaft."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
