{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12ae9e64-0e6d-45a6-97bd-038fd31e5b7e",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "#### Single cell effects\n",
    "This aggregates information across a relatively small number of pre-synaptic cells and a large number of post-synaptic cells.  Might the relationship between connection and correlation vary with pre or post-synaptic cell? If certain cells are more visually responsive, or are leaders vs followers, or chorists vs soloists, this might lead to very different synaptic plasticity.  Checking whether individual cells (either pre or post) have more shifted distributions than others could provide some evidence for this.  Keep in mind that the number of connection reconstructed will effect the statistical power you have to detect changes of different sizes.\n",
    "\n",
    "In this exercise, you will need to use \n",
    "pandas groupby\n",
    "pandas merge\n",
    "scipy.stats.pearsonr\n",
    "matplotlib plotting\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24a46018-85b7-4546-9428-b47e70d20302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import caveclient\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4d39f69-1463-452c-a45a-173d90fe7226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import os\n",
    "\n",
    "platstring = platform.platform()\n",
    "if ('Darwin' in platstring) or ('macOS' in platstring):\n",
    "    # macOS \n",
    "    data_root = \"/Volumes/Brain2023/\"\n",
    "elif 'Windows'  in platstring:\n",
    "    # Windows (replace with the drive letter of USB drive)\n",
    "    data_root = \"E:/\"\n",
    "elif ('amzn' in platstring):\n",
    "    # then on Code Ocean\n",
    "    data_root = \"/data/\"\n",
    "else:\n",
    "    # then your own linux platform\n",
    "    # EDIT location where you mounted hard drive\n",
    "    data_root = \"/media/$USERNAME/Brain2023/\"\n",
    "    \n",
    "data_dir = os.path.join(data_root, 'microns_in_silico')\n",
    "\n",
    "# you can just override this if the location of the data varies\n",
    "# data_dir = '/Users/forrestc/Downloads/microns_in_silico/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa0e8809-d33d-4243-9a5d-b05b63b8bd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are going to load up the data and prepare the dataframe like we did \n",
    "# in class but with fewer comments\n",
    "\n",
    "# load up the in-silico responses as a pandas dataframe from a numpy array \n",
    "resp=pd.DataFrame(np.load(os.path.join(data_dir, 'nat_resp.npy')))\n",
    "\n",
    "# load up the csv of metadata about the 104171 units\n",
    "units_df = pd.read_csv(os.path.join(data_dir, 'nat_unit.csv'))\n",
    "\n",
    "# set the index to the be the row_idx of the units_df\n",
    "resp.index = units_df['row_idx']\n",
    "\n",
    "# if we are on code ocean, the CAVEsetup helped you make your token an environment variable\n",
    "if 'amzn' in platstring:\n",
    "    client= caveclient.CAVEclient('minnie65_public', auth_token=os.environ['API_SECRET'])\n",
    "else:\n",
    "    # otherwise if you are local, then it should be saved to a file in your harddrive \n",
    "    # that the caveclient knows where to read.\n",
    "    client= caveclient.CAVEclient('minnie65_public')\n",
    "\n",
    "# lets pull out the manual coregistered neurons\n",
    "# desired_resolution describes how many nanometers you want each unit to be\n",
    "# so 1000,1000,1000 gives positions in microns for x,y and z\n",
    "coreg_df = client.materialize.query_table('coregistration_manual_v3', desired_resolution=[1000,1000,1000])\n",
    "\n",
    "# lets merge these dataframes so we get the row_idx of each coregistered unit\n",
    "# we merge on the corresponding columns, however scan was called something\n",
    "# slightly different in one csv vs the CAVE table\n",
    "coreg_in_silico=pd.merge(units_df, coreg_df, \n",
    "         left_on=['scan_session', 'scan_idx', 'unit_id'],\n",
    "          right_on=['session','scan_idx', 'unit_id'])\n",
    "# reset the index to make sure that we have the index\n",
    "coreg_in_silico.reset_index(inplace=True)\n",
    "\n",
    "# this will pull out the responses to the coregistered units\n",
    "# by using the row_idx that was provided in the metadata\n",
    "coreg_resp = resp.loc[coreg_in_silico.row_idx,:]\n",
    "\n",
    "# now with a reduced set of units, we can calculate the Pearson correlation\n",
    "# between their responses\n",
    "corr_M = np.corrcoef(coreg_resp.values)\n",
    "\n",
    "ct_df = client.materialize.query_table('aibs_soma_nuc_exc_mtype_preds_v117')\n",
    "# lets merge it on the coregistered cells with in silico responses\n",
    "# we will use the segment version is (pt_root_id) to do this\n",
    "ct_merge_df=pd.merge(coreg_in_silico.reset_index(),\n",
    "                     ct_df[['pt_root_id', 'id_ref', 'cell_type']],\n",
    "                     on='pt_root_id')\n",
    "\n",
    "# lets pull all the neurons where we can trust the axons\n",
    "# as being reasonably well reconstructed\n",
    "prf_df=client.materialize.query_table('proofreading_status_public_release', \n",
    "                                      filter_in_dict={'status_axon': ['extended', 'clean']})\n",
    "\n",
    "# how many of these are coregistered?\n",
    "clean_coreg_df = pd.merge(prf_df, coreg_in_silico, on='pt_root_id')\n",
    "\n",
    "# we need this code to work in solutions directory\n",
    "# and one up..\n",
    "if 'solutions' in os.getcwd():\n",
    "    workshop2file = '../../workshop2/all_prf_coreg_conn_v661.pkl'\n",
    "else:\n",
    "    workshop2file = '../workshop2/all_prf_coreg_conn_v661.pkl'\n",
    "all_syn_df = pd.read_pickle(workshop2file)\n",
    "\n",
    "nuc_df = client.materialize.query_view('nucleus_detection_lookup_v1', \n",
    "                                        select_columns = ['id', 'pt_root_id', 'pt_position'],\n",
    "                                        desired_resolution=[1000,1000,1000])\n",
    "\n",
    "# lets merge on the pre and post-synaptic positions of these connections\n",
    "\n",
    "# renaming the positions as pre and post depending on how we did the merge\n",
    "# and drop the duplicate id columns\n",
    "all_syn_dfm=all_syn_df.merge(nuc_df[['id', 'pt_position']], left_on='pre_nuc_id', right_on='id')\\\n",
    ".rename({'pt_position':'pre_pt_position'}, axis=1)\\\n",
    ".merge(nuc_df[['id', 'pt_position']], left_on='post_nuc_id', right_on='id')\\\n",
    ".rename({'pt_position':'post_pt_position'}, axis=1)\\\n",
    ".drop(['id_x', 'id_y'], axis=1)\n",
    "\n",
    "# now lets merge in the neurons that are coregistered with responses\n",
    "\n",
    "# we have to drop duplicates to avoid the few cells that were coregistered twice \n",
    "# being double counted\n",
    "all_syn_dfm2=all_syn_dfm.merge(coreg_in_silico[['index','target_id', 'scan_session', 'scan_idx', 'field','unit_id', 'score', 'residual']],\n",
    "                  left_on='pre_nuc_id', \n",
    "                  right_on='target_id')\\\n",
    ".merge(coreg_in_silico[['index','target_id', 'scan_session', 'scan_idx', 'field','unit_id','score', 'residual']],\n",
    "                  left_on='post_nuc_id', \n",
    "                  right_on='target_id',\n",
    "                  suffixes=['_pre', '_post'])\\\n",
    ".drop(['target_id_pre', 'target_id_post'],axis=1)\\\n",
    ".drop_duplicates(subset=['pre_nuc_id', 'post_nuc_id'])\n",
    "all_syn_dfm2\n",
    "\n",
    "# now use fancy indexing to pull out the correlation associated with each of these connections\n",
    "all_syn_dfm2['C']=corr_M[all_syn_dfm2.index_pre, all_syn_dfm2.index_post]\n",
    "\n",
    "\n",
    "# now lets merge in our cell type calls\n",
    "# by using suffixes we will name the pre and post synaptic cell type \n",
    "# differently\n",
    "all_syn_dfm3=all_syn_dfm2.merge(ct_df[['target_id', 'cell_type']],\n",
    "                  left_on='pre_nuc_id', \n",
    "                  right_on='target_id')\\\n",
    ".merge(ct_df[['target_id', 'cell_type']],\n",
    "                  left_on='post_nuc_id', \n",
    "                  right_on='target_id',\n",
    "                  suffixes=['_pre', '_post'])\\\n",
    ".drop(['target_id_pre', 'target_id_post'],axis=1)\\\n",
    ".drop_duplicates(subset=['pre_nuc_id', 'post_nuc_id'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ff20852-28e3-4b97-a2fa-c17901712199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pre_pt_root_id</th>\n",
       "      <th>post_pt_root_id</th>\n",
       "      <th>n_syn</th>\n",
       "      <th>sum_size</th>\n",
       "      <th>C</th>\n",
       "      <th>cell_type_pre</th>\n",
       "      <th>cell_type_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>864691135927049742</td>\n",
       "      <td>864691136310417242</td>\n",
       "      <td>1</td>\n",
       "      <td>1732</td>\n",
       "      <td>0.169119</td>\n",
       "      <td>L5ET</td>\n",
       "      <td>L5ET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>864691136228183377</td>\n",
       "      <td>864691136310417242</td>\n",
       "      <td>1</td>\n",
       "      <td>7604</td>\n",
       "      <td>0.010837</td>\n",
       "      <td>L4c</td>\n",
       "      <td>L5ET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>864691135155894884</td>\n",
       "      <td>864691136310417242</td>\n",
       "      <td>1</td>\n",
       "      <td>9404</td>\n",
       "      <td>0.040026</td>\n",
       "      <td>L2a</td>\n",
       "      <td>L5ET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>864691135591041291</td>\n",
       "      <td>864691136310417242</td>\n",
       "      <td>2</td>\n",
       "      <td>25900</td>\n",
       "      <td>0.145264</td>\n",
       "      <td>L2a</td>\n",
       "      <td>L5ET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864691136194822888</td>\n",
       "      <td>864691136310417242</td>\n",
       "      <td>1</td>\n",
       "      <td>9972</td>\n",
       "      <td>0.191115</td>\n",
       "      <td>L4a</td>\n",
       "      <td>L5ET</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pre_pt_root_id     post_pt_root_id  n_syn  sum_size         C  \\\n",
       "0  864691135927049742  864691136310417242      1      1732  0.169119   \n",
       "1  864691136228183377  864691136310417242      1      7604  0.010837   \n",
       "2  864691135155894884  864691136310417242      1      9404  0.040026   \n",
       "3  864691135591041291  864691136310417242      2     25900  0.145264   \n",
       "4  864691136194822888  864691136310417242      1      9972  0.191115   \n",
       "\n",
       "  cell_type_pre cell_type_post  \n",
       "0          L5ET           L5ET  \n",
       "1           L4c           L5ET  \n",
       "2           L2a           L5ET  \n",
       "3           L2a           L5ET  \n",
       "4           L4a           L5ET  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets check we got the same thing\n",
    "all_syn_dfm3[['pre_pt_root_id', 'post_pt_root_id', 'n_syn',\n",
    "              'sum_size', 'C', 'cell_type_pre', 'cell_type_post']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba1c6878-618c-45b1-8360-81509bfac22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the connections by pre synaptic cell\n",
    "\n",
    "# look at the top 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e4da7ec-8ed8-4da9-a231-1b785972bf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the average correlation and standard deviations of correlations for each individual cell\n",
    "# hint: use group by\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "167dccf9-820a-4d0d-92cb-d7c1dab96c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine these into a dataframe\n",
    "# label each of your series using X.name=\"series name\"\n",
    "# before combining them into a dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18529087-5439-4a5a-8f5b-a2f476c8124f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# make a scatter plot of the mean correlation vs the number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b64b9c9f-3990-4031-bc6b-c55457e1dbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a scatter plot of the mean correlation vs the standard deviation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "032f3ac8-4ff8-4235-8c57-7d3df5e74a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now do the same thing by post-synaptic cell\n",
    "\n",
    "# count the connections by post synaptic cell\n",
    "\n",
    "# look at the top 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c469ebd-7d70-4ccf-806a-3268d432efeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the average correlation and standard deviations of correlations for each individual cell\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "372d4b95-0321-4589-99bf-12c36eceaf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a scatter plot of the mean correlation vs the number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "013c1506-3542-44fd-875c-776ca95f5c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# are there any cells that have a lot of mapped inputs and outputs?\n",
    "\n",
    "# combine the pre and post dataframes into one\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c6ea602-b0a9-4fb1-bc8b-2ad27d922bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a scatter plot of the N of inputs and outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b0ab75e-3f04-432c-960c-7ef388c7208b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out the cells with low N from both conditions\n",
    "# and make a scatter plot of the avg mean correlation of inputs vs outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2ced4a0-b224-4a20-8f20-759c7d35a443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the correlation between those values and its P value?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea38a4dc-dffc-47e9-acb4-cae65a8d747a",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "#### Follow-up questions - extensions\n",
    "\n",
    "1. What might explain this variation from cell to cell?\n",
    "   Biological ideas\n",
    "   a) Cell types.  Do you see these high and low effect cells more in certain cell types?  What is a good null model for this?\n",
    "   b) Different connectivity in the heirachy.  Do cells that axon project in different directions correlate with this effect?\n",
    "   c) Different levels of inhibition.  Perhaps inhibitory circuits are somehow modulating plasticity in these cells.\n",
    "      Does the amount on total excitatory or inhibitory input vary in these high vs low cells.\n",
    "    \n",
    "   Technical issues\n",
    "   a) Reconstruction quality.  Are the axons of the more correlated cells better reconstructed?\n",
    "      Examining the reconstructions of the low effect vs high effect cells could give insight into this.\n",
    "   b) Model fit.  Does the in silico response accurately reflect this cells firing?\n",
    "      Assessing whether the oracle score, or repeating the analysis on raw data, or with orientation tuning\n",
    "      Might show this\n",
    "   \n",
    "   Hybrid issues\n",
    "   a) Space: See space exercise.  What if cells in certain regions show this effect more or less,\n",
    "      what if synapses that are farther or closer to the cell are more prone to this effect?\n",
    "   b) Synapse size: see size exercise. What if the reconstructions from some cells have more larger synapses than others\n",
    "   and synapse size is a strong correlated of this effect? \n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
